{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import googleapiclient.discovery\n",
    "import csv\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# ===== Configuration Variables =====\n",
    "API_KEY = 'GOOGLE-API-KEY'  # Enter your actual API key here\n",
    "\n",
    "# Batch name (to distinguish multiple crawling jobs)\n",
    "BATCH_NAME = 'default'  # e.g., 'january_2024', 'test_batch', 'main_collection', etc.\n",
    "\n",
    "# List of video IDs to crawl\n",
    "VIDEO_IDS = [\n",
    "    '_1gluMtaUmg',\n",
    "    'o69BiOqY1Ec',\n",
    "    'ZnmsMg6joGo',\n",
    "]\n",
    "\n",
    "# Output directory (separated by batch)\n",
    "OUTPUT_DIR = f'comments/{BATCH_NAME}/'\n",
    "\n",
    "# Progress tracking file (separated by batch)\n",
    "PROGRESS_FILE = f'crawling_progress_{BATCH_NAME}.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_progress(video_id, next_page_token=None, completed=False):\n",
    "    \"\"\"Save crawling progress to CSV file\"\"\"\n",
    "    try:\n",
    "        # Load existing data\n",
    "        progress = load_progress()\n",
    "        \n",
    "        # Update current video info\n",
    "        progress[video_id] = {\n",
    "            'video_id': video_id,\n",
    "            'next_page_token': next_page_token or '',\n",
    "            'completed': completed,\n",
    "            'last_updated': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Save to CSV file\n",
    "        with open(PROGRESS_FILE, 'w', encoding='utf-8', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['video_id', 'next_page_token', 'completed', 'last_updated'])\n",
    "            \n",
    "            for data in progress.values():\n",
    "                writer.writerow([\n",
    "                    data['video_id'],\n",
    "                    data['next_page_token'],\n",
    "                    data['completed'],\n",
    "                    data['last_updated']\n",
    "                ])\n",
    "        \n",
    "        print(f\"Progress saved: {video_id} - {'Completed' if completed else 'In Progress'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save progress: {e}\")\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Load saved crawling progress from CSV\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(PROGRESS_FILE):\n",
    "            progress = {}\n",
    "            with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                for row in reader:\n",
    "                    video_id = row['video_id']\n",
    "                    progress[video_id] = {\n",
    "                        'video_id': video_id,\n",
    "                        'next_page_token': row['next_page_token'] if row['next_page_token'] else None,\n",
    "                        'completed': row['completed'].lower() == 'true',\n",
    "                        'last_updated': row['last_updated']\n",
    "                    }\n",
    "            return progress\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load progress: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "\n",
    "def print_progress_status():\n",
    "    \"\"\"Print current progress status\"\"\"\n",
    "    progress = load_progress()\n",
    "    print(f\"\\n=== Batch '{BATCH_NAME}' Crawling Progress ===\")\n",
    "    \n",
    "    for video_id in VIDEO_IDS:\n",
    "        if video_id in progress:\n",
    "            status = progress[video_id]\n",
    "            if status.get('completed', False):\n",
    "                print(f\"{video_id}: Completed\")\n",
    "            else:\n",
    "                token = status.get('next_page_token')\n",
    "                print(f\"{video_id}: In Progress (Token: {token[:20] + '...' if token else 'None'})\")\n",
    "        else:\n",
    "            print(f\"{video_id}: Not Started\")\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replies(parent_id, writer, api_key, depth):\n",
    "    # Build the YouTube API client\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        \"youtube\", \"v3\", developerKey=api_key\n",
    "    )\n",
    "\n",
    "    request = youtube.comments().list(\n",
    "        part=\"snippet\",\n",
    "        parentId=parent_id,\n",
    "        maxResults=100\n",
    "    )\n",
    "\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response.get(\"items\", []):\n",
    "            reply = item[\"snippet\"]\n",
    "            reply_id = item[\"id\"]\n",
    "            writer.writerow([\n",
    "                reply_id,\n",
    "                reply[\"authorDisplayName\"],\n",
    "                reply.get(\"authorChannelId\", {}).get(\"value\"),\n",
    "                reply[\"textDisplay\"],\n",
    "                reply[\"likeCount\"],\n",
    "                reply[\"publishedAt\"],\n",
    "                depth,\n",
    "                parent_id\n",
    "            ])\n",
    "\n",
    "            # Fetch deeper replies if available\n",
    "            get_replies(reply_id, writer, api_key, depth + 1)\n",
    "\n",
    "        request = youtube.comments().list_next(request, response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_youtube_comments(video_id, writer, api_key, next_page_token=None):\n",
    "    \"\"\"Crawl YouTube comments and save progress\"\"\"\n",
    "    # Build the YouTube API client\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        \"youtube\", \"v3\", developerKey=api_key\n",
    "    )\n",
    "    \n",
    "    print(f\"Crawling started - Video ID: {video_id}\")\n",
    "    if next_page_token:\n",
    "        print(f\"Resuming from previous stop point - Token: {next_page_token[:20]}...\")\n",
    "    \n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"videoId\": video_id,\n",
    "        \"maxResults\": 100,\n",
    "    }\n",
    "    \n",
    "    if next_page_token:\n",
    "        params[\"pageToken\"] = next_page_token\n",
    "\n",
    "    request = youtube.commentThreads().list(**params)\n",
    "    page_count = 0\n",
    "\n",
    "    try:\n",
    "        while request:\n",
    "            response = request.execute()\n",
    "            page_count += 1\n",
    "            \n",
    "            print(f\"Processing page {page_count}... ({len(response.get('items', []))} comments)\")\n",
    "\n",
    "            for item in response.get(\"items\", []):\n",
    "                comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "                comment_id = item[\"snippet\"][\"topLevelComment\"][\"id\"]\n",
    "                writer.writerow([\n",
    "                    comment_id,\n",
    "                    comment[\"authorDisplayName\"],\n",
    "                    comment.get(\"authorChannelId\", {}).get(\"value\"),\n",
    "                    comment[\"textDisplay\"],\n",
    "                    comment[\"likeCount\"],\n",
    "                    comment[\"publishedAt\"],\n",
    "                    0,\n",
    "                    None\n",
    "                ])\n",
    "\n",
    "                # Fetch replies for the comment\n",
    "                get_replies(comment_id, writer, api_key, depth=1)\n",
    "            \n",
    "            # Check for next page token\n",
    "            next_token = response.get(\"nextPageToken\")\n",
    "            if next_token:\n",
    "                print(f\"Next page token: {next_token[:20]}...\")\n",
    "                # Save progress (intermediate save)\n",
    "                save_progress(video_id, next_token, completed=False)\n",
    "            else:\n",
    "                print(\"All pages completed!\")\n",
    "                # Save as completed\n",
    "                save_progress(video_id, None, completed=True)\n",
    "                break\n",
    "\n",
    "            request = youtube.commentThreads().list_next(request, response)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during crawling: {e}\")\n",
    "        # Save current token on error\n",
    "        if 'next_token' in locals() and next_token:\n",
    "            save_progress(video_id, next_token, completed=False)\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_title(video_id, api_key):\n",
    "    # Build the YouTube API client\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        \"youtube\", \"v3\", developerKey=api_key\n",
    "    )\n",
    "\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet\",\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    items = response.get(\"items\", [])\n",
    "    if items:\n",
    "        return items[0][\"snippet\"][\"title\"].replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "    else:\n",
    "        return \"UnknownTitle\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_videos():\n",
    "    \"\"\"Crawl comments for all videos (always resumes from where it stopped)\"\"\"\n",
    "    print(\"YouTube Comment Crawling Started!\")\n",
    "    print(f\"Batch name: {BATCH_NAME}\")\n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "    print(f\"Progress file: {PROGRESS_FILE}\")\n",
    "    print(f\"Number of videos to process: {len(VIDEO_IDS)}\")\n",
    "    \n",
    "    print_progress_status()\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    success_count = 0\n",
    "    for i, video_id in enumerate(VIDEO_IDS, 1):\n",
    "        print(f\"\\n[{i}/{len(VIDEO_IDS)}] Processing...\")\n",
    "        \n",
    "        try:\n",
    "            print(f\"Starting video processing: {video_id}\")\n",
    "            \n",
    "            # Get video title\n",
    "            video_title = get_video_title(video_id, API_KEY)\n",
    "            filename = f\"{OUTPUT_DIR}{video_title}_{video_id}.csv\"\n",
    "            \n",
    "            # Check if resuming\n",
    "            progress = load_progress()\n",
    "            next_page_token = None\n",
    "            if video_id in progress and not progress[video_id].get('completed', False):\n",
    "                next_page_token = progress[video_id].get('next_page_token')\n",
    "                if next_page_token:\n",
    "                    print(f\"Resuming from previous stop point\")\n",
    "                else:\n",
    "                    print(f\"Starting new crawling\")\n",
    "            else:\n",
    "                print(f\"Starting new crawling\")\n",
    "            \n",
    "            # Determine file mode (append for resume, write for new)\n",
    "            file_mode = \"a\" if next_page_token else \"w\"\n",
    "            \n",
    "            with open(filename, file_mode, encoding=\"utf-8\", newline=\"\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                \n",
    "                # Write header for new crawling\n",
    "                if not next_page_token:\n",
    "                    writer.writerow([\"ID\", \"Author\", \"AuthorID\", \"Comment\", \"LikeCount\", \"PublishedAt\", \"Depth\", \"ParentID\"])\n",
    "                \n",
    "                # Start comment crawling\n",
    "                get_youtube_comments(video_id, writer, API_KEY, next_page_token)\n",
    "            \n",
    "            print(f\"Completed: {filename}\")\n",
    "            success_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred ({video_id}): {e}\")\n",
    "    \n",
    "    print(f\"\\nCrawling completed!\")\n",
    "    print_progress_status()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Usage =====\n",
    "\n",
    "# Check current batch info\n",
    "print(f\"Current batch: {BATCH_NAME}\")\n",
    "print_progress_status()\n",
    "\n",
    "# To start crawling, uncomment the following:\n",
    "crawl_videos()  # Always resumes from where it stopped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_progress_status()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
